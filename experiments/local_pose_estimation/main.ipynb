{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from ssc import ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_files = sorted(glob.glob(\"frames/frame_*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [cv2.imread(frame_file, cv2.IMREAD_COLOR) for frame_file in frame_files[:300]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f119fe265d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(frames[0][:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1920\n",
    "h = 1080\n",
    "fx = 1184.51770\n",
    "fy = 1183.63810\n",
    "cx = 978.30778\n",
    "cy = 533.85598\n",
    "camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "fast = cv2.FastFeatureDetector_create(threshold=12)\n",
    "num_ret_points = 3000\n",
    "tolerance = 0.1\n",
    "num_matches = int(0.8*num_ret_points)  # keep only best 80 % of the matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kp_des(frame, fast, orb):\n",
    "    kp = fast.detect(frame, None)\n",
    "    kp = sorted(kp, key = lambda x:x.response, reverse=True)\n",
    "    kp = ssc(kp, num_ret_points, tolerance, frame.shape[1], frame.shape[0])\n",
    "    kp, des = orb.compute(frame, kp)\n",
    "    # good features to track lead to cleaner tracks, but much more noisy pose estimates\n",
    "    #kp = cv2.goodFeaturesToTrack(frame, **feature_params)\n",
    "    #kp = cv2.KeyPoint_convert(kp)\n",
    "    #kp, des = orb.compute(frame, kp)\n",
    "    return kp, des\n",
    "\n",
    "\n",
    "def match(bf, last_keyframe, current_frame, last_des, des, last_kp, kp, distance_threshold=25.0, draw=True):\n",
    "    matches = bf.match(last_des, des)\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    # filter out matches with distance (descriptor appearance) greater than threshold\n",
    "    matches = [m for m in matches if m.distance < distance_threshold]\n",
    "    print(\"Found {} matches of current frame with last frame\".format(len(matches)))\n",
    "    last_pts = np.array([last_kp[m.queryIdx].pt for m in matches]).reshape(1, -1, 2)\n",
    "    current_pts = np.array([kp[m.trainIdx].pt for m in matches]).reshape(1, -1, 2)\n",
    "    match_frame = np.zeros_like(current_frame)\n",
    "    if draw:\n",
    "        match_frame = cv2.drawMatches(last_keyframe, last_kp, current_frame, kp, matches[:250], None)\n",
    "    return matches, last_pts, current_pts, match_frame\n",
    "\n",
    "\n",
    "def to_twist(R, t):\n",
    "    \"\"\"Convert a 3x3 rotation matrix and translation vector (shape (3,))\n",
    "    into a 6D twist coordinate (shape (6,)).\"\"\"\n",
    "    r, _ = cv2.Rodrigues(R)\n",
    "    twist = np.zeros((6,))\n",
    "    twist[:3] = r.reshape(3,)\n",
    "    twist[3:] = t.reshape(3,)\n",
    "    return twist\n",
    "\n",
    "\n",
    "def from_twist(twist):\n",
    "    \"\"\"Convert a 6D twist coordinate (shape (6,)) into a 3x3 rotation matrix\n",
    "    and translation vector (shape (3,)).\"\"\"\n",
    "    r = twist[:3].reshape(3, 1)\n",
    "    t = twist[3:].reshape(3, 1)\n",
    "    R, _ = cv2.Rodrigues(r)\n",
    "    return R, t\n",
    "\n",
    "\n",
    "def get_frame(cap, mapx, mapy):\n",
    "    \"\"\"Reads and undistorts next frame from stream.\"\"\"\n",
    "    retc, frame = cap.read()\n",
    "    if not retc:\n",
    "        raise RuntimeError(\"Could not read the first camera frame.\")\n",
    "    frame = cv2.remap(frame, mapx, mapy, cv2.INTER_CUBIC)  # undistort frame\n",
    "    return frame\n",
    "\n",
    "\n",
    "def gray(frame):\n",
    "    \"\"\"Convert BGR frame to gray frame.\"\"\"\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return frame_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialization is not very robust and only works for some special cases, need to change to homography initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(fast, orb, camera_matrix, min_parallax=60.0):\n",
    "    \"\"\"Initialize two keyframes, the camera poses and a 3D point cloud.\n",
    "\n",
    "    Args:\n",
    "        min_parallax (`float`): Threshold for the median distance of all\n",
    "            keypoint matches between the first keyframe (firs frame) and the\n",
    "            second key frame. Is used to determine which frame is the second\n",
    "            keyframe. This is needed to ensure enough parallax to recover\n",
    "            the camera poses and 3D points.\n",
    "    \"\"\"\n",
    "    pose_graph = nx.Graph()  # stores keyframe poses and data (keypoints, ORB descriptors, etc.)\n",
    "    map_points = np.empty(shape=(0, 3), dtype=np.float64)\n",
    "\n",
    "    # get first key frame\n",
    "    frame = frames[0]\n",
    "    kp, des = extract_kp_des(gray(frame), fast, orb)\n",
    "    pose_graph.add_node(0, frame=frame, kp=kp, des=des)\n",
    "\n",
    "    frame_idx_init = 0\n",
    "\n",
    "    for frame in frames[1:]:\n",
    "        \n",
    "        frame_idx_init += 1\n",
    "\n",
    "        # extract keypoints and match with first key frame\n",
    "        kp, des = extract_kp_des(gray(frame), fast, orb)\n",
    "        matches, last_pts, current_pts, match_frame = match(bf,\n",
    "            gray(pose_graph.nodes[0][\"frame\"]), gray(frame), pose_graph.nodes[0][\"des\"],\n",
    "            des, pose_graph.nodes[0][\"kp\"], kp, num_matches, draw=False)\n",
    "\n",
    "        # determine median distance between all matched feature points\n",
    "        median_dist = np.median(np.linalg.norm(last_pts.reshape(-1, 2)-current_pts.reshape(-1, 2), axis=1))\n",
    "        print(median_dist)\n",
    "\n",
    "        # if distance exceeds threshold choose frame as second keyframe\n",
    "        if median_dist >= min_parallax:# and frame_idx_init >= 20:\n",
    "            pose_graph.add_node(1, frame=frame, kp=kp, des=des)\n",
    "            break\n",
    "\n",
    "    pose_graph.add_edge(0, 1, matches=matches)\n",
    "\n",
    "    # separately store the keypoints in matched order for tracking later\n",
    "    pose_graph.nodes[0][\"kp_idx\"] = np.array([m.queryIdx for m in matches])\n",
    "    pose_graph.nodes[1][\"kp_idx\"] = np.array([m.trainIdx for m in matches])\n",
    "\n",
    "    # compute relative camera pose for second frame\n",
    "    essential_mat, _ = cv2.findEssentialMat(last_pts.reshape(1, -1, 2), current_pts.reshape(1, -1, 2), camera_matrix, method=cv2.LMEDS)  # RANSAC fails here\n",
    "    num_inliers, R, t, mask = cv2.recoverPose(essential_mat, last_pts.reshape(1, -1, 2), current_pts.reshape(1, -1, 2), camera_matrix)\n",
    "    mask = mask.astype(np.bool).reshape(-1,)\n",
    "    print(num_inliers)\n",
    "\n",
    "    if num_inliers >= 0.25*current_pts.reshape(1, -1, 2).shape[1]:\n",
    "        print(\"init R\", R)\n",
    "        print(\"init t\", t)\n",
    "\n",
    "        # relative camera pose\n",
    "        R1 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]).astype(np.float64)\n",
    "        t1 = np.array([[0], [0], [0]]).astype(np.float64)\n",
    "        R2 = R.T\n",
    "        t2 = -np.matmul(R.T, t.reshape(3,)).reshape(3,1)\n",
    "\n",
    "        # insert pose (in twist coordinates) of KF0 and KF1 into keyframes dict\n",
    "        # poses are w.r.t. KF0 which is the base coordinate system of the entire map\n",
    "        pose_graph.nodes[0][\"pose\"] = to_twist(R1, t1)\n",
    "        pose_graph.nodes[1][\"pose\"] = to_twist(R2, t2)\n",
    "\n",
    "        # create projection matrices needed for triangulation of initial 3D point cloud\n",
    "        proj_matrix1 = np.hstack([R1.T, -R1.T.dot(t1)])\n",
    "        proj_matrix2 = np.hstack([R2.T, -R2.T.dot(t2)])\n",
    "        proj_matrix1 = camera_matrix.dot(proj_matrix1)\n",
    "        proj_matrix2 = camera_matrix.dot(proj_matrix2)\n",
    "\n",
    "        # triangulate initial 3D point cloud\n",
    "        pts_3d = cv2.triangulatePoints(proj_matrix1, proj_matrix2, last_pts.reshape(-1, 2).T, current_pts.reshape(-1, 2).T).T\n",
    "        pts_3d = cv2.convertPointsFromHomogeneous(pts_3d).reshape(-1, 3)\n",
    "\n",
    "        # filter outliers based on mask from recoverPose\n",
    "        #pts_3d = pts_3d[valid_map_points_mask, :].reshape(-1, 3)\n",
    "\n",
    "        # add triangulated points to map points\n",
    "        map_points = np.vstack((map_points, pts_3d))  # map_points stores 3D points w.r.t. KF0\n",
    "        \n",
    "        #map_points_info.append({  # stores keypoints and ORB descriptors associated to each map point\n",
    "        #    0: ([pose_graph.nodes[0][\"kp\"][m.queryIdx] for m in pose_graph.edges[(0, 1)][\"matches\"]],\n",
    "        #        [pose_graph.nodes[0][\"des\"][m.queryIdx] for m in pose_graph.edges[(0, 1)][\"matches\"]]), \n",
    "        #    1: ([pose_graph.nodes[1][\"kp\"][m.trainIdx] for m in pose_graph.edges[(0, 1)][\"matches\"]],\n",
    "        #        [pose_graph.nodes[1][\"des\"][m.trainIdx] for m in pose_graph.edges[(0, 1)][\"matches\"]])})\n",
    "\n",
    "        # store indices of map points belonging to KF0 and KF1 in pose graph node\n",
    "        pose_graph.nodes[0][\"associated_map_points\"] = np.arange(0, pts_3d.shape[0])\n",
    "        pose_graph.nodes[1][\"associated_map_points\"] = np.arange(0, pts_3d.shape[0])\n",
    "\n",
    "        print(\"Initialization successful. Chose frames 0 and {} as key frames\".format(frame_idx_init))\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"Could not recover intial camera pose based on selected keyframes. Insufficient parallax or number of feature points.\")\n",
    "\n",
    "    # TODO: perform full BA to optimize initial camera poses and map points [see. ORB_SLAM IV. 5)]\n",
    "\n",
    "    return pose_graph, map_points, frame_idx_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1324 matches of current frame with last frame\n",
      "3.0\n",
      "Found 1422 matches of current frame with last frame\n",
      "7.0\n",
      "Found 1311 matches of current frame with last frame\n",
      "10.04987562112089\n",
      "Found 1411 matches of current frame with last frame\n",
      "13.038404810405298\n",
      "Found 1302 matches of current frame with last frame\n",
      "17.0\n",
      "Found 1333 matches of current frame with last frame\n",
      "20.024984394500787\n",
      "Found 1286 matches of current frame with last frame\n",
      "23.08679276123039\n",
      "Found 1337 matches of current frame with last frame\n",
      "26.076809620810597\n",
      "Found 1261 matches of current frame with last frame\n",
      "30.066592756745816\n",
      "Found 1299 matches of current frame with last frame\n",
      "33.06055050963308\n",
      "Found 1304 matches of current frame with last frame\n",
      "36.124783736376884\n",
      "Found 1292 matches of current frame with last frame\n",
      "40.049968789001575\n",
      "Found 1288 matches of current frame with last frame\n",
      "43.104524124504614\n",
      "Found 1282 matches of current frame with last frame\n",
      "47.042533945356304\n",
      "Found 1287 matches of current frame with last frame\n",
      "50.039984012787215\n",
      "Found 1268 matches of current frame with last frame\n",
      "53.150729063673246\n",
      "Found 1292 matches of current frame with last frame\n",
      "56.22277118748239\n",
      "Found 1233 matches of current frame with last frame\n",
      "60.07495318350236\n",
      "1123\n",
      "init R [[ 9.99999062e-01  1.20892064e-03 -6.43091627e-04]\n",
      " [-1.20936538e-03  9.99999030e-01 -6.91632076e-04]\n",
      " [ 6.42254874e-04  6.92409160e-04  9.99999554e-01]]\n",
      "init t [[-0.05216726]\n",
      " [-0.99835129]\n",
      " [-0.02394329]]\n",
      "Initialization successful. Chose frames 0 and 18 as key frames\n"
     ]
    }
   ],
   "source": [
    "pose_graph, map_points, frame_idx = initialize(fast, orb, camera_matrix, min_parallax=60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 1230, 1231, 1232])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_graph.nodes[0][\"associated_map_points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.31761475,  1.45177197, 19.94844452],\n",
       "       [ 5.00779483,  7.47543074, 20.33402537],\n",
       "       [-8.40211766, -0.74192502, 20.49784261],\n",
       "       ...,\n",
       "       [-0.44074669,  1.12982152, -5.63570685],\n",
       "       [-0.30234757,  0.61048963, -1.0331949 ],\n",
       "       [ 0.05750397,  0.55473557,  0.28256981]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get map points associated to a KF in the same order as the feature points\n",
    "map_points[pose_graph.nodes[0][\"associated_map_points\"], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get KF features & descriptors associated to map points in a KF\n",
    "features_0 = [pose_graph.nodes[0][\"kp\"][idx] for idx in pose_graph.nodes[0][\"kp_idx\"]]\n",
    "features_1 = [pose_graph.nodes[1][\"kp\"][idx] for idx in pose_graph.nodes[1][\"kp_idx\"]]\n",
    "descriptors_0 = [pose_graph.nodes[0][\"des\"][idx] for idx in pose_graph.nodes[0][\"kp_idx\"]]\n",
    "descriptors_1 = [pose_graph.nodes[1][\"des\"][idx] for idx in pose_graph.nodes[1][\"kp_idx\"]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "diff = cv2.KeyPoint_convert(features_1) - cv2.KeyPoint_convert(features_0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "img0 = cv2.drawKeypoints(np.copy(pose_graph.nodes[0][\"frame\"]), features_0, None)\n",
    "img1 = cv2.drawKeypoints(np.copy(pose_graph.nodes[1][\"frame\"]), features_1, None)\n",
    "ax1.imshow(img0[:, :, ::-1])\n",
    "ax2.imshow(img1[:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_camera_pose(img_points, pts_3d, camera_matrix):\n",
    "    \"\"\"Estimates the camera world pose of a frame based on 2D-3D\n",
    "    corresponding points.\n",
    "\n",
    "    Args:\n",
    "        img_points (`numpy.ndarray`): A set of keypoints extracted from the\n",
    "            current frame. Shape (-1, 1, 2). These keypoints can also be tracked\n",
    "            from the previous frame.\n",
    "\n",
    "        pts_3d (`numpy.ndarray`): Triangulated 3D points corresponding to\n",
    "            the keypoints in img_points. Shape (-1, 1, 3). Note, that the order\n",
    "            of the points in this array needs to be consistent with the order of\n",
    "            keypoints in img_points.\n",
    "\n",
    "        camera_matrix (`numpy.ndarray`): Camera matrix of the camera which\n",
    "            was used to acquire frames.\n",
    "\n",
    "    Returns:\n",
    "        R (`numpy.ndarray`): Rotation matrix of the camera coordinate system\n",
    "            w.r.t. world coordinate system. Shape (3, 3).\n",
    "        t (`numpy.ndarray`): Translation (x, y, z) of the camera coordinate\n",
    "            system w.r.t. world coordinate system. Shape (3,).\n",
    "\n",
    "    Note:\n",
    "        This function assumes keypoints to be extracted from an undistorted\n",
    "        frame.\n",
    "    \"\"\"\n",
    "    success, rvec, tvec, inliers = cv2.solvePnPRansac(pts_3d.reshape(-1, 1, 3), img_points.reshape(-1, 1, 2), camera_matrix, None, reprojectionError=8, iterationsCount=100)\n",
    "    if not success:\n",
    "        raise RuntimeError(\"Could not compute the camera pose for the new frame with solvePnP.\")\n",
    "    print(\"solvePnP success\", success)\n",
    "    print(\"solvePnP inliers\", inliers.shape)\n",
    "    R = cv2.Rodrigues(rvec)[0].T\n",
    "    t = -np.matmul(cv2.Rodrigues(rvec)[0].T, tvec)\n",
    "    return R, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_node_id = sorted(pose_graph.nodes)[-1]\n",
    "last_kf = pose_graph.nodes[prev_node_id][\"frame\"]\n",
    "current_frame = frames[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1206 matches of current frame with last frame\n"
     ]
    }
   ],
   "source": [
    "# match current frame with last key frame\n",
    "current_kp, current_des = extract_kp_des(gray(current_frame), fast, orb)\n",
    "matches, last_pts, current_pts, match_frame = match(bf,\n",
    "        gray(last_kf),\n",
    "        gray(current_frame),\n",
    "        pose_graph.nodes[prev_node_id][\"des\"],\n",
    "        current_des, pose_graph.nodes[prev_node_id][\"kp\"],\n",
    "        current_kp, 25.0, draw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find feature points in last KF associated with the new matches\n",
    "prev_node_id = sorted(pose_graph.nodes)[-1]\n",
    "last_kps = [pose_graph.nodes[prev_node_id][\"kp\"][m.queryIdx] for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1206"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_kps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1206, 2)\n",
      "[[13.31761475  1.45177197 19.94844452]\n",
      " [ 5.00779483  7.47543074 20.33402537]\n",
      " [-8.40211766 -0.74192502 20.49784261]\n",
      " ...\n",
      " [-0.44074669  1.12982152 -5.63570685]\n",
      " [-0.30234757  0.61048963 -1.0331949 ]\n",
      " [ 0.05750397  0.55473557  0.28256981]]\n",
      "(1233, 3)\n",
      "float64\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) /io/opencv/modules/calib3d/src/solvepnp.cpp:216: error: (-215:Assertion failed) npoints >= 4 && npoints == std::max(ipoints.checkVector(2, CV_32F), ipoints.checkVector(2, CV_64F)) in function 'solvePnPRansac'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-7e6ee3f55cf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts_3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(\"current_pts before PnP\", img_points, \"len\", img_points.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mR_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_camera_pose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts_3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcamera_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mcurrent_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_twist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_current\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_current\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-ba672b0d035c>\u001b[0m in \u001b[0;36mestimate_camera_pose\u001b[0;34m(img_points, pts_3d, camera_matrix)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \"\"\"\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minliers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolvePnPRansac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts_3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcamera_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreprojectionError\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterationsCount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not compute the camera pose for the new frame with solvePnP.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.2.0) /io/opencv/modules/calib3d/src/solvepnp.cpp:216: error: (-215:Assertion failed) npoints >= 4 && npoints == std::max(ipoints.checkVector(2, CV_32F), ipoints.checkVector(2, CV_64F)) in function 'solvePnPRansac'\n"
     ]
    }
   ],
   "source": [
    "img_points = cv2.KeyPoint_convert(last_kps)\n",
    "print(img_points.shape)\n",
    "last_map_points = map_points[pose_graph.nodes[prev_node_id][\"associated_map_points\"], :]  # may contain multiple disconnected index ranges\n",
    "pts_3d = last_map_points\n",
    "\n",
    "print(pts_3d)\n",
    "print(pts_3d.shape)\n",
    "print(pts_3d.dtype)\n",
    "#print(\"current_pts before PnP\", img_points, \"len\", img_points.shape)\n",
    "R_current, t_current = estimate_camera_pose(img_points, pts_3d, camera_matrix)\n",
    "current_pose = to_twist(R_current, t_current)\n",
    "print(R_current, t_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n",
      "479\n"
     ]
    }
   ],
   "source": [
    "print(matches[0].queryIdx)  # index of feature point in the last_kf for match[0]\n",
    "print(matches[0].trainIdx)  # index of feature point in the current_frame for match[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(last_kf[:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f1fffaec264ee6a6f2efb537a9899e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(match_frame[:, :, ::-1])\n",
    "cv2.imwrite(\"match_frame.jpg\", match_frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project map points into image\n",
    "rvec = \n",
    "tvec = \n",
    "imagePoints, jacobian = cv.projectPoints(objectPoints, rvec, tvec, camera_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f38ec15e810>]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([m.distance for m in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: keep only matches with distance smaller than threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1522 matches of current frame with last frame\n",
      "(1, 1522, 2)\n",
      "pts3d\n",
      "[[13.31761475  1.45177197 19.94844452]\n",
      " [ 5.00779483  7.47543074 20.33402537]\n",
      " [-8.40211766 -0.74192502 20.49784261]\n",
      " ...\n",
      " [-0.44074669  1.12982152 -5.63570685]\n",
      " [-0.30234757  0.61048963 -1.0331949 ]\n",
      " [ 0.05750397  0.55473557  0.28256981]]\n",
      "(1233, 3)\n",
      "float64\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) /io/opencv/modules/calib3d/src/solvepnp.cpp:216: error: (-215:Assertion failed) npoints >= 4 && npoints == std::max(ipoints.checkVector(2, CV_32F), ipoints.checkVector(2, CV_64F)) in function 'solvePnPRansac'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-07f525a393f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts_3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#print(\"current_pts before PnP\", img_points, \"len\", img_points.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mR_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_camera_pose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts_3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcamera_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mcurrent_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_twist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_current\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_current\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-ba672b0d035c>\u001b[0m in \u001b[0;36mestimate_camera_pose\u001b[0;34m(img_points, pts_3d, camera_matrix)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \"\"\"\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minliers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolvePnPRansac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts_3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcamera_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreprojectionError\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterationsCount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not compute the camera pose for the new frame with solvePnP.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.2.0) /io/opencv/modules/calib3d/src/solvepnp.cpp:216: error: (-215:Assertion failed) npoints >= 4 && npoints == std::max(ipoints.checkVector(2, CV_32F), ipoints.checkVector(2, CV_64F)) in function 'solvePnPRansac'\n"
     ]
    }
   ],
   "source": [
    "for current_frame in frames[frame_idx+1:]:\n",
    "    # get initial pose estimate by matching keypoints with previous KF\n",
    "    current_kp, current_des = extract_kp_des(gray(current_frame), fast, orb)\n",
    "    prev_node_id = sorted(pose_graph.nodes)[-1]\n",
    "    matches, last_pts, current_pts, match_frame = match(bf,\n",
    "        gray(pose_graph.nodes[prev_node_id][\"frame\"]),\n",
    "        gray(current_frame),\n",
    "        pose_graph.nodes[prev_node_id][\"des\"],\n",
    "        current_des, pose_graph.nodes[prev_node_id][\"kp\"],\n",
    "        current_kp, num_matches, draw=False)\n",
    "\n",
    "    vis_current_frame = cv2.drawKeypoints(np.copy(current_frame), current_kp, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "    # recover initial camera pose of current frame by solving PnP\n",
    "    img_points = current_pts\n",
    "    print(img_points.shape)\n",
    "    visible_map_points = pose_graph.nodes[prev_node_id][\"visible_map_points\"]  # may contain multiple disconnected index ranges\n",
    "    try:\n",
    "        pts_3d = np.vstack([map_points[vs, :] for vs in visible_map_points])\n",
    "    except ValueError:\n",
    "        raise ValueError (\"Last keyframe did not contain any visible map points.\")\n",
    "\n",
    "    print(\"pts3d\")\n",
    "    print(pts_3d)\n",
    "    print(pts_3d.shape)\n",
    "    print(pts_3d.dtype)\n",
    "    #print(\"current_pts before PnP\", img_points, \"len\", img_points.shape)\n",
    "    R_current, t_current = estimate_camera_pose(img_points, pts_3d, camera_matrix)\n",
    "    current_pose = to_twist(R_current, t_current)\n",
    "    print(R_current, t_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
